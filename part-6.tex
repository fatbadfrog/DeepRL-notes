\part{Stochastic Approximation} This part lays the necessary foundation for learning Temporal-Difference Methods. We
reconsider the mean estimation problem, suppose there is a sequence of i.i.d samples $\{x_{i}\}_{i=1}^{n}$, the
expectation of X can be approximated by:
\begin{equation}
  \label{ch6:1}
  E[X] \approx \overline{x} \doteq \frac{1}{n}\sum_{i=1}^{n}x_{i}
\end{equation}

There are two methods to solve \ref{ch6:1}:
\begin{enumerate}

  \item
        Non-incremental method. This is a standard MC method that collects all the samples first and then calculates the
        average. For an accurate estimation, a sufficient number of samples should be collected, which may require a
        significant amount of time.
  \item
        Incremental method. To avoid the drawbacks of non-incremental method, this method calculates the average incrementally
        instead of waiting for all samples. Specially,
        \begin{equation*}
          \omega_{k+1} \doteq \frac{1}{k}\sum_{i=1}^{k}x_{i}
        \end{equation*}
        and hence
        \begin{equation*}
          \omega_{k} = \frac{1}{k-1}\sum_{i=1}^{k-1}x_{i}
        \end{equation*}
        The $\omega_{k+1}$ can be expressed as:
        \begin{equation}
          \label{ch6:2}
          \omega_{k+1}=\frac{1}{k}(\sum_{i=1}^{k-1}x_{i}+x_{k}) = \frac{1}{k}\left[(k-1) \omega_{k}+x_{k}\right]=
          \omega_{k}-\frac{1}{k}(\omega_{k}-x_{k})
        \end{equation}

\end{enumerate}
The advantage of \ref{ch6:2} is that average can be calculated immediately upon receiving a sample. As more samples are
obtained, the accuracy of estimation can be gradually improved according to the law of large numbers. Moreover,
replacing $k$ in \ref{ch6:2} is replaced with $\alpha$ results in the main algorithm we will discuss in this part.
\begin{equation}
  \label{ch6:3}
  \omega_{k+1}=\omega_{k}-\alpha(\omega_{k}-x_{k})
\end{equation}

\section{Robbins-Monro algorithm}

  Algorithm \ref{ch6:3} is a typical representation of the stochastic approximation method. In fact, the term "stochastic
  approximation" refers to a broad class of \emph{stochastic iterative} algorithms for solving root-finding or
  optimization problems. Compared to many other root-finding algorithms, stochastic approximation is powerful because it
  does not require knowledge of objective functions or their derivatives. This section will introduce the famous
  Robbins-Monro (RM) algorithm. \par

  Suppose we want to find the root of the equation:
  \begin{equation*}
    g(\omega) = 0
  \end{equation*}
  Before introducing the details of RM algorithm, it is essential to note that root-finding problem is significant as many
  other problems can be formulated into its form. For example, if we want to find the maximum or minimum of $J(\omega)$,
  we can let $g(\omega) = \nabla J(\omega)$ and solve the equation $g(\omega)=0$ which is a root-finding problem. \par As
  mentioned above, RM is powerful because it does not require the expression of objective function and its derivatives
  \sn{for instance, function represented by neural networks whose structure and parameters are unknown}. Even the function
  has a noisy observation, RM algorithm still solve the problem. Suppose that noisy observation of $g(\omega)$ is
  expressed as:
  \begin{equation*}
    \tilde{g}(\omega) = g(\omega) + \eta
  \end{equation*}
  where $\eta$ is the observation error, which is Gaussian or not. \par The RM algorithm that solve $g(\omega)=0$ is:
  \begin{equation}
    \label{rm_algorithm}
    \omega_{k+1}=\omega_{k}-a_{k}\tilde{g}(\omega)
  \end{equation}
  The reason for RM algorithm can find the root is very simple. That is {\color{myred}{$\ \omega_{k+1} \text{ is closer to
          } \omega^{*} \\ \text{than } \omega_{k}$}}. The explanation is listed below:
  \begin{enumerate}

    \item
          If $\omega_{k}>\omega^{*}$, then $\tilde{g}(\omega)>0$. Then
          $\omega_{k+1}=\omega_{k}-a_{k}\tilde{g}(\omega)<\omega_{k}$. If $a_{k}\tilde{g}(\omega)$ is sufficient small, we have
          $\omega^{*}<\omega_{k+1}<\omega_{k}$.
    \item
          If $\omega_{k}<\omega^{*}$, then $\tilde{g}(\omega)<0$. Then
          $\omega_{k+1}=\omega_{k}-a_{k}\tilde{g}(\omega)>\omega_{k}$. If $a_{k}\tilde{g}(\omega)$ is sufficient small, we have
          $\omega_{k}<\omega_{k+1}<\omega^{*}$.

  \end{enumerate}

  In either case, $\omega_{k+1}$ is closer to $\omega^{*}$ than $\omega_{k}$. A rigorous convergence result is given
  below:
  \begin{theorem}[Robbins-Monro theorem]
    \label{rm:1}
    In the Robbins-Monro algorithm \ref{rm_algorithm}, if
    \begin{enumerate}
      \item
            $0<c_{1} \le \nabla_{\omega}g(\omega) \le c_{2}$ for all $\omega$;
      \item
            $\sum_{k=1}^{\infty}a_{k}=\infty \ \text{and} \ \sum_{k=1}^{\infty}a_{k}^{2} < \infty$
      \item
            $E[\eta_{k}| \mathcal{H}_{k}] = 0 \ \text{and} \ E[\eta^{2}|\mathcal{H}_{k}]<\infty$
    \end{enumerate}
    where $\mathcal{H}_{k}= \{\omega_{k},\omega_{k-1},\cdots \}$, then $\omega_{k}$ almost surely converges to
    $\omega^{*}$ satisfying $g(\omega^{*})=0$.
  \end{theorem}

  The three conditions \sn{$a_{k}=\frac{1}{k}$ satisfies these conditions} in theorem \ref{rm:1} are explained as follows:
  \begin{enumerate}
    \item
          $0<c_{1} \le \nabla_{\omega}g(\omega) \le c_{2}$ for all $\omega$. This condition indicates that $g(\omega)$ is a
          monotonically increasing function which guarantee that $g(\omega)=0$ has a unique root. $\nabla g(\omega) \le c_{2}$
          indicates that the gradient of $g(\omega)$ is bounded from above.
    \item
          The second condition is common in reinforcement learning. $\sum_{k=1}^{\infty}a_{k}^{2} < \infty$ requires $a_{k}$
          converges to zero as $k \to \infty$, whereas $\sum_{k=1}^{\infty}a_{k}=\infty$ requires that $a_{k}$ should not
          converge to zero too fast. Here is insight into why $\{a_{k}\}$ should be configured this way. First, suppose that the
          observation is bounded. Since
          \begin{equation*}
            \omega_{k+1} - \omega_{k} = -a_{k}\tilde{g_{k}}(\omega_{k},\eta_{k})
          \end{equation*}
          if $a_{k}\to \infty$, $a_{k}\to 0$ and hence $-a_{k}\tilde{g_{k}}(\omega_{k},\eta_{k})$ which means
          $\omega_{k+1}-\omega_{k} \to 0$. Otherwise, if $\{a_{k}\}$ not converges to zero, $\omega_{k}$ will fluctuate. Second,
          summarizing both sides of every equation of $\omega_{2} - \omega_{1} = -a_{1}\tilde{g_{1}}(\omega_{1},\eta_{1}),
            \omega_{3} - \omega_{2} = -a_{k}\tilde{g_{2}}(\omega_{2},\eta_{2}), \cdots$, gives
          \begin{equation*}
            \omega_{\infty} - \omega_{1} = -\sum_{k=1}^{\infty}a_{k}\tilde{g_{k}}(\omega_{k},\eta_{k})
          \end{equation*}
          if $\sum_{k=1}^{\infty}<\infty$, then $\left|\sum_{k=1}^{\infty}a_{k}\tilde{g_{k}}(\omega_{k},\eta_{k})\right|$ is
          also bounded. Let b denotes the upper bound such that
          \begin{equation*}
            \left|\omega_{\infty} - \omega_{1}\right| = \left|\sum_{k=1}^{\infty}a_{k}\tilde{g_{k}}(\omega_{k},\eta_{k})\right|
            \le b
          \end{equation*}
          Now you will see if the initial guess value $\omega_{1}$ is far away from the true value
          ($\left|\omega_{1}-\omega^{*}\right|>b$), $\omega_{\infty}$ will never converge to $\omega^{*}$.
  \end{enumerate}

\subsection{Application to mean estimation}

  Using RM algorithm to solve mean estimation, the key point is to find $\tilde{g}(\omega)$, which is
  $\tilde{g}(\omega)=g(\omega)+\eta$. The $g(\omega)$ is obvious if the estimation problem is formulated to root-finding.
  Since we want to find an $\omega$ equals to E[X], the $g(\omega)$ can be defined as:
  \begin{equation*}
    g(\omega) = \omega - E[X]
  \end{equation*}
  \par We know that $\tilde{g}(\omega)$ is $g(\omega)$ plus a noisy observation which is the difference between the
  observation value of random variable X and E[X], so $\tilde{g}(\omega)$ can be defined as:
  \begin{align*}
    \tilde{g}(\omega) & = g(\omega) + \eta                                                            \\
                      & = \omega - E[X] + \underbrace{\colorbox{myred!20!white}{$(E[X] - x)$}}_{\eta} \\
                      & = \omega - x
  \end{align*}
  Now, we can use RM algorithm
  \begin{equation*}
    \omega_{k+1} = \omega_{k} - \alpha_{k}\tilde{g}(\omega_{k}) = \omega_{k} - \alpha_{k}(\omega_{k}-x_{k})
  \end{equation*}
  to find the root of $g(\omega)$ which is the E[X]. \sn{$\alpha_{k}$ can be set to $\frac{1}{k}$}

\section{Dvoretzky's converge theorem}

  To prove the convergence of RM algorithm, we use Dvoretzky's converge theorem, which is
  \begin{theorem}{(Dvoretzky's theorem)}
    \label{thm:dvoretzky}
    Considering a stochastic process:
    \begin{equation*}
      \Delta_{k+1} = \left(1-\alpha_k\right)\Delta_{k+1}+\beta_k\eta_k
    \end{equation*}
    where \( \{\alpha_k \}_{k=1}^{\infty}, \{\beta_k\}_{k=1}^{\infty} \) and \( \{\eta_k\}_{k=1}^{\infty} \) are
    stochastic sequences. Here \( \alpha_k \ge 0, \beta_k \ge 0 \) for all k. Then \( \Delta_k \) converges to zero almost
    surely if the following conditions are satisfied.
    \begin{enumerate}[label=\alph*)]
      \item
            \( \sum_{k=1}^{\infty} \alpha_k =\infty, \sum_{k=1}^{\infty} \alpha_{k}^{2}< \infty, and \sum_{k=1}^{\infty}
            \beta_k^{2}<\infty\) are uniformly almost surely.
      \item
            $\mathbb{E}[\eta_{k}|\mathcal{H}_{k}]=0$ and $\mathbb{E}[\eta^{2}|\mathcal{H}_{k}] \le C$ almost surely.\\ where
            $\mathcal{H}_{k}=\{\Delta_k,\Delta_{k-1},\cdots,\eta_{k-1},\cdots,\alpha_{k-1},\cdots,\beta_{k-1},\cdots\}$
    \end{enumerate}
  \end{theorem}
  Before proving this theorem, something need to be clarified,
  \begin{enumerate}

    \item
          In RM algorithm, $\{\alpha_{k}\}$ and $\{\beta_{k}\}$ is deterministic, whereas in Dvoretzky's theorem, they can be
          random variables which depend on \{$\mathcal{H}_{k}$\}. So the first condition is stated as "uniformly almost surely".
          \sn{Uniformly almost surely usually describes the convergence of stochastic sequence. It should be noted that the sum
            of $\{\alpha_{k}\}$ and $\{\beta_{k}\}$ are also stochastic sequence.}
    \item
          Theorem \ref{thm:dvoretzky} does not require $\sum_{k=1}^{\infty} \beta_{k}=\infty$. In fact, in the extreme case in
          which $\beta_{k}=0$ for all k, the sequence $\{\Delta_{k}\}$ can still converge.

  \end{enumerate}
  We prove Theorem \ref{thm:dvoretzky} using quasimartingale converge theorem (\ref{thm:quasimartingale}).
  \begin{proof}
    Let $h_{k} \triangleq \Delta_{k}^{2}$, then we have,
    \begin{align*}
      h_{k+1} - h_{k} & = \Delta_{k+1}^{2} - \Delta_{k}^{2}                                                                                     \\
                      & = (\Delta_{k+1}+\Delta_{k})(\Delta_{k+1}-\Delta_{k})                                                                    \\
                      & = \left[(2-\alpha_{k}) \Delta_{k}+\beta_{k} \eta_{k}\right](-\alpha_{k} \Delta_{k}+\beta_{k} \eta_{k})                  \\
                      & = -\alpha_{k}(2-\alpha_{k}) \Delta_{k}^{2} + \beta_{k}^{2} \eta_{k}^{2} + 2(1-\alpha_{k}) \beta_{k} \eta_{k} \Delta_{k}
    \end{align*}
    Taking exception on both sides of the above equation, we have \[\mathbb{E}[h_{k+1} - h_{k}|\mathcal{H}_{k}] =
      \mathbb{E}[-\alpha_{k}(2-\alpha_{k}) \Delta_{k}^{2}|\mathcal{H}_{k}] + \mathbb{E}[\beta_{k}^{2}
        \eta_{k}^{2}|\mathcal{H}_{k}] + \mathbb{E}[2(1-\alpha_{k}) \beta_{k} \eta_{k} \Delta_{k}|\mathcal{H}_{k}] \]
    According to the conditions stated in Theorem \ref{thm:dvoretzky}, the sequences $\{\alpha_{k}\}$ and $\{\beta_{k}\}$
    either rely on $\mathcal{H}_{k}$ or are deterministic. Then, they can be taken out of the expectation.(Lemma
    \ref{le2}(e)) Therefore, we have
    \begin{equation}
      \label{equ:hk}
      \mathbb{E}[h_{k+1} - h_{k}|\mathcal{H}_{k}] = -\alpha_{k}(2-\alpha_{k}) \Delta_{k}^{2} + \beta_{k}^{2}
      \mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}] + 2(1-\alpha_{k}) \beta_{k} \Delta_{k}\mathbb{E}[\eta_{k} |\mathcal{H}_{k}]
    \end{equation}
    Since the condition implies $\alpha_{k} \to 0$ almost surely as $k\to \infty$, there exists a finite n such that
    $\{\alpha_{k}\} \le 1$ for all $k\ge n$ almost surely. Without losing generality, we merely consider the case of
    $\alpha_{k} \le 1$. Then, $-\alpha_{k}(2-\alpha_{k}) \Delta_{k}^{2} <0$. As assumed, the second term $\beta_{k}^{2}
      \mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}] \le \beta_{k}^{2}C$ and the third term equals to zero. Hence, \[
      \mathbb{E}[h_{k+1} - h_{k}|\mathcal{H}_{k}] \le \beta_{k}^{2}C \] and \[\sum_{k=1}^{\infty}{\mathbb{E}[h_{k+1} -
        h_{k}|\mathcal{H}_{k}]} \le \sum_{k=1}^{\infty}{\beta_{k}^{2}C}\] Now according to quasimartingale converge
    theorem (\ref{thm:quasimartingale}), $\{\Delta_{k}\}$ converges almost surely. Next, we need to determine which value
    $\{\Delta_{k}\}$ converges to. Based on equation (\ref{equ:hk})
    \begin{equation*}
      \sum_{k=1}^{\infty}{\alpha_{k}(2-\alpha_{k}) \Delta_{k}^{2}} = \sum_{k=1}^{\infty}{\beta_{k}^{2}
      \mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}]} - \sum_{k=1}^{\infty} {\mathbb{E}[h_{k+1} - h_{k}|\mathcal{H}_{k}]}
    \end{equation*}
    \\ Both terms of right side are bounded and hence $\sum_{k=1}^{\infty}{\alpha_{k}(2-\alpha_{k}) \Delta_{k}^{2}} $ is
    also bounded. Since we merely consider the case of $\alpha_{k} \le 1$, then
    \begin{equation*}
      \infty > \sum_{k=1}^{\infty}{\alpha_{k}(2-\alpha_{k}) \Delta_{k}^{2}} \ge \sum_{k=1}^{\infty}{\alpha_{k}
      \Delta_{k}^{2}} > 0
    \end{equation*}
    Therefore $\sum_{k=1}^{\infty}{\alpha_{k} \Delta_{k}^{2}}$ is bounded. Since $\sum_{k=1}^{\infty}{\alpha_{k}}=\infty$,
    we must have $\Delta_{k} \to 0$ almost surely.
  \end{proof}

\subsection{Application to mean estimation}

  Since we have proved the convergence of Dvoretzky's theorem, the convergence of mean estimation method can be directly
  proven.
  \begin{proof}
    Let $\omega^{*}=\mathbb{E}[X]$ then the mean estimation algorithm
    $\omega_{k+1}=\omega_{k}+\alpha_{k}(x_{k}-\omega_{k})$ can be rewritten as
    \begin{equation*}
      \omega_{k+1}-\omega^{*} = \omega_{k}-\omega^{*} +\alpha_{k}(x_{k} - \omega^{*} + \omega^{*} -\omega_{k})
    \end{equation*}
    Then if we define $\Delta_k=\omega_{k} - \omega^{*}$, we will get the Dvoretzky's theorem form:
    \begin{equation*}
      \Delta_{k+1} = (1-\alpha)\Delta_{k} + \alpha_{k}\underbrace{(x_{k}-\omega^{*})}_{\eta}
    \end{equation*}
    If we choose suitable $\alpha_{k}$, the convergence is only determined by $\mathbb{E}[\eta_{k}|\mathcal{H}_{k}]$ and
    $\mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}]$. So we can compute them and have
    \begin{align*}
      \mathbb{E}[\eta_{k}|\mathcal{H}_{k}] & = \mathbb{E}[x_{k}|\mathcal{H}_{k}] - \mathbb{E}[\omega^{*}|\mathcal{H}_{k}]        \\
                                           & = \mathbb{E}[x_{k}|\mathcal{H}_{k}] - \omega^{*} \quad (\{x_{k}\} \text{ is i.i.d}) \\
                                           & = \mathbb{E}[X] - \omega^{*}                                                        \\
                                           & = 0
    \end{align*}
    \begin{align*}
      \mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}] & = \mathbb{E}[x_{k}^{2}|\mathcal{H}_{k}] - 2\omega^{*}\times \mathbb{E}[x_{k}|\mathcal{H}_{k}]+\omega^{*2} \\
                                               & = \mathbb{E}[x_{k}]^{2} + Var[x_{k}] - \omega^{*2} \quad (\{x_{k}\} \text{ is i.i.d})                     \\
                                               & = Var[x_{k}]
    \end{align*}
    From above equations, we know that $\mathbb{E}[\eta_{k}|\mathcal{H}_{k}]=0$ and
    $\mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}] $ is bounded if $Var[x_{k}]$ is finite. So following Dvoretzky's theorem, we
    can conclude $\Delta_{k}$ converges to zero and $\omega_{k}$ almost surely converge to $\omega^{*}=\mathbb{E}[X]$.
  \end{proof}

\subsection{Application to RM algorithm}

  The key point is to define $\Delta_{k}$. According to RM algorithm \sn{Remember RM is a root-finding algorithm and
    $g(\omega^{*})=0$}, we can rewrite the iterative formula as:
  \begin{align*}
    \omega_{k+1} - \omega^{*} & = \omega_{k} - \omega^{*} - a_{k}[g(\omega_{k})+\eta_{k}]                                                                      \\
    \Delta_{k+1}              & = \Delta_{k} - a_{k}[g(\omega_{k})-g(\omega^{*})+\eta_{k}] \quad (\text{Define } \Delta_{k} \text{ as } \omega_{k}-\omega^{*})
  \end{align*}
  According to mean value theorem, $g(\omega_{k})-g(\omega^{*})=\nabla_{\omega} g(\omega)(\omega_{k}-\omega^{*})$, we get
  \begin{align*}
    \Delta_{k+1} & = \Delta_{k} - a_{k}[\nabla_{\omega} g(\omega)(\omega_{k}-\omega^{*})+\eta_{k}]                                           \\
                 & = (1-\underbrace{a_{k}\nabla_{\omega} g(\omega)}_{\colorbox{myyellow!80!myred}{$\alpha_{k}$}}) \Delta_{k} + a_{k}\eta_{k}
  \end{align*}
  As RM algorithm assumed, $0<c_{1}\le \nabla_{\omega} g(\omega) \le c_{2}$ is bounded, and since
  $\sum_{k=1}^{\infty}a_{k}=\infty$ and $\sum_{k=1}^{\infty}a_{k}^{2}<\infty$, $\alpha_{k}$ satisfies the condition of
  Dvoretzky's theorem. Thus, all conditions satisfy Dvoretzky's theorem requirements \sn {conditions of $\{\eta_{k}\}$ in
    RM algorithm is as same as Dvoretzky's theorem} and hence $\Delta_{k}$ converges to zero which means that $\omega_{k}
    \to \omega^{*}$ as $k\to \infty$.

\subsection{An extension of Dvoretzky's theorem}

  This is an extension of Dvoretzky's theorem for handling multiple variables and can be used to analyze the convergence
  of Q-learning.
  \begin{theorem}
    Considering a finite set $\mathcal{S}$ of real numbers. For the stochastic process,
    \begin{equation}
      \label{eq:dvo-extension}
      \Delta_{k+1}(s) = (1-\alpha_{k}(s)) \Delta_{k}(s) + \beta_{k}(s)\eta_{k}(s)
    \end{equation}
    it holds that $\Delta_{k}(s)$ converges to zero almost surely for every $s\in \mathcal{S}$ if the following conditions
    are satisfied:
    \begin{enumerate}[label=(\alph*)]
      \item
            $\sum_{k=1}^{\infty} \alpha_{k}(s)=\infty$, $\sum_{k=1}^{\infty} \alpha_{k}^{2}(s)<\infty$, $\sum_{k=1}^{\infty}
              \beta_{k}^{2}(s)<\infty$, and $\mathbb{E}[\beta_{k}(s)|\mathcal{H}_{k}]\le
              \mathbb{E}[\alpha_{k}(s)|\mathcal{H}_{k}]$ uniformly almost surely.
      \item
            $\Vert\mathbb{E}[\eta_{k}(s)|\mathcal{H}_{k}]\Vert_{\infty} \le \gamma\Vert \Delta_{k}\Vert_{\infty}$, where $\gamma
              \in (0,1)$
      \item
            $var[\eta_{k}(s)|\mathcal{H}_{k}] \le C(1+\Vert \Delta_{k}(s)\Vert_{\infty})^{2}$, where $C$ is a constant.
    \end{enumerate}
    Here $\mathcal{H}_{k}=\{\Delta_{k}, \Delta_{k-1},\cdots,\eta_{k-1},\cdots,\alpha_{k-1},\cdots,\beta_{k-1},\cdots\}$
    represents the historical information and $\Vert \cdot \Vert_{\infty}$ is maximum norm.
  \end{theorem}
  We do not prove this theorem and just give some remarks below
  \begin{enumerate}

    \item
          The variables $s$ can be reviewed as index. In the context of reinforcement learning, it indicates the state or the
          state-action pair. The maximum norm $\Vert \cdot \Vert_{\infty}$ is defined over set, which means
          $\Vert\mathbb{E}[\eta_{k}(s)|\mathcal{H}_{k}]\Vert_{\infty}=\max_{s\in
              \mathcal{S}}\left|\mathbb{E}[\eta_{k}(s)|\mathcal{H}_{k}]\right|$
    \item
          This theorem can handle the reinforcement learning problem with multiple states because it uses $\Vert \cdot
            \Vert_{\infty}$. It only requires the expectation and variance of $\eta_{k}(s)$ is bounded by $\Delta_{k}(s)$.
    \item
          Convergence of $\Delta_{k}(s)$ for all $s\in \mathcal{S}$ requires that the conditions are valid for all $s \in
            \mathcal{S}$, that is for all states or state-cation pairs.

  \end{enumerate}

\section{Stochastic gradient descent}

  RM is a root-finding algorithm, and we will introduce an optimization algorithm which is widely used in machine leaning
  field. The algorithm is called Stochastic gradient descent, and we will see it is a special RM algorithm. Considering
  the following optimization problem:
  \begin{equation*}
    \min_{\omega}J(\omega) = \mathbb{E}[f(\omega,X)]
  \end{equation*}
  where $\omega$ is parameters to be optimized, and $X$ is a random variable. Here $\omega$ and $X$ can be either scalars
  or vectors. The function $f( cdot)$ is a scalar. A straight method for solving this problem is gradient descent method,
  which is
  \begin{align}
    \label{eq:gd-expectation}
    \omega_{k+1} & = \omega_{k} - \alpha_{k} \nabla_{\omega}\mathbb{E}[f(\omega,X)] \\
                 & = \omega_{k} - \alpha_{k}\mathbb{E}[\nabla_{\omega}f(\omega,X)]
  \end{align}
  In practice, we usually do not know the distribution of $X$, one way to estimate the expectation is to collect numerous
  i.i.d samples $\{x_{i}\}_{i=1}^{\infty}$ of $X$ and the approximated value is
  \begin{equation*}
    \mathbb{E}[\nabla_{\omega}f(\omega,X)]\approx \frac{1}{n}\sum_{i=1}^{n}\nabla_{\omega}f(\omega,x_{i})
  \end{equation*}
  Then equation (\ref{eq:gd-expectation}) can be rewritten as
  \begin{equation*}
    \omega_{k+1} = \omega_{k} - \alpha_{k}\frac{1}{n}\sum_{i=1}^{n}\nabla_{\omega}f(\omega,x_{i})
  \end{equation*}
  One problem with above algorithm is we can not collect so many samples at one time. If the sample is collected one by
  one, it is favorable to update $\omega$ every time a sample is collected. So we should use algorithm below:
  \begin{equation}
    \label{eq:SGD}
    \omega_{k+1} = \omega_{k} - \alpha_{k}\nabla_{\omega}f(\omega,x_{i})
  \end{equation}
  where $x_{k}$ is the sample collected at step $k$. This is the well known stochastic gradient descent (SGD) algorithm.
  The term "stochastic" denotes that the algorithm relies on the stochastic samples $\{x_{i}\}$. One may immediately doubt
  the correctness of SGD because $\nabla_{\omega}f(\omega,x_{i}) \neq \mathbb{E}[\nabla_{\omega}f(\omega,X)]$. Now we give
  the proof on the convergence of SGD.\sn{SGD is an optimization algorithm which find $\omega^{*}$ to minimize the target
    function $J(\omega)=\mathbb{E}[f(\omega,X)]$. It is equivalent to find the root of $\nabla_{\omega}
      \mathbb{E}[f(\omega,X)]=0$}.

  \begin{theorem}{Convergence of SGD}
    \label{thm:SGD}
    For the SGD algorithm in (\ref{eq:SGD}), if the following conditions are satisfied, then $\omega_{k}$ converges to the
    root of $\nabla_{\omega} \mathbb{E}[f(\omega,X)]=0$ almost surely.
    \begin{enumerate}
      [label=(\alph*)]

      \item
            $0<c_{1} \le \nabla_{\omega}^{2}f(\omega,X) \le c_{2}$;
      \item
            $\sum_{k=1}^{\infty}a_{k}=\infty$ and $\sum_{k=1}^{\infty}a_{k}^{2}<\infty$;
      \item
            $\{x_{k}\}_{k=1}^{\infty}$ are i.i.d.

    \end{enumerate}

  \end{theorem}
  Condition (a) is about the convexity of $f$. It requires curvature of $f$ to be bounded from above and below. Here
  $\omega$ is a scalar, and so is $\nabla_{\omega}^{2}f(\omega,X)$ is. When $\omega$ is a vector,
  $\nabla_{\omega}^{2}f(\omega,X)$ is the well-known Hessian matrix.

  \begin{proof}
    Let $g(\omega)=\nabla_{\omega}J(\omega)=\nabla_{\omega}\mathbb{E}[f(\omega,X)]=\mathbb{E}[\nabla_{\omega}f(\omega,X)]$
    We use the RM algorithm and have
    \begin{equation*}
      \omega_{k+1} = \omega_{k} - a_{k}\tilde{g}(\omega_{k},x_{k})
    \end{equation*}
    Now we should find the form of $\eta_{k}$. It should be noticed that if we can get all sample values at step $k$, we
    can directly use $g(\omega)$ not $\tilde{g}(\omega)$ to update $\omega_{k+1}$. But in SGD algorithm, the sample value
    is collected one by one, so what we have is $\nabla_{\omega}f(\omega,x_{k})$ and a "noise"($\eta_{k}$) which is the
    difference between the observation and true system output is generated. According to the definition of
    $\tilde{g}(\omega)$, we have
    \begin{align*}
      \tilde{g}(\omega_{k}.x_{k}) & = g(\omega_{k}) + \eta_{k}                                                                                                                         \\
                                  & = \mathbb{E}[\nabla_{\omega}f(\omega_{k},X)] + \underbrace{\nabla_{\omega}f(\omega_{k},x_{k}) - \mathbb{E}[\nabla_{\omega}f(\omega_{k},X)]}_{\eta} \\
                                  & = \nabla_{\omega}f(\omega_{k},x_{k})
    \end{align*}
    and the final form of RM algorithm for solving $g(\omega)=0$ is
    \begin{equation}
      \label{eq:RM-SGD}
      \omega_{k+1} = \omega_{k} - a_{k}\nabla_{\omega}f(\omega_{k},x_{k})
    \end{equation}
    and this form also is SGD algorithm. If we can conform equation (\ref{eq:RM-SGD}) satisfies conditions in theorem
    (\ref{rm:1}), convergence of SGD can be guaranteed. Since
    $\nabla_{\omega}g(\omega)=\mathbb{E}[\nabla_{\omega}^{2}f(\omega,X)]$ and $\nabla_{\omega}^{2}f(\omega,X)$ is bounded
    according to this theorem, $c_{1} \le \nabla_{\omega}g(\omega) \le c_{2}$ is satisfied. \par The second condition in
    theorem (\ref{rm:1}) is the same as this theorem. \par For the third condition in theorem (\ref{rm:1}), we first
    compute $\mathbb{E}[\eta_{k}|\mathcal{H}_{k}]$.
    \begin{equation*}
      \mathbb{E}[\eta_{k}|\mathcal{H}_{k}] = \mathbb{E}[\nabla_{\omega}f(\omega_{k}, x_{k})|\mathcal{H}_{k}] -
      \mathbb{E}[\mathbb{E}[\nabla_{\omega}f(\omega_{k},X)]|\mathcal{H}_{k}]
    \end{equation*}
    Because $\{x_{k}\}_{k=1}^{\infty}$ is i.i.d and independent of $\mathcal{H}_{k}=\{\omega_{k},\omega_{k-1},\cdots\}$,
    the first term of the right side of the equation above is
    $\mathbb{E}_{x_{k}}[\nabla_{\omega}f(\omega,x_{k})]=\mathbb{E}[\nabla_{\omega}f(\omega_{k},X)]$. For the second term,
    the inside expectation gives a function of $\omega_{k}$ so that this term equals to \marginnote{$\omega_{k}$ is an
      element of $\mathcal{H}_{k}$ and $\mathbb{E}[f(Y)|Y]=f(Y)$} $\mathbb{E}[\nabla_{\omega}f(\omega_{k},X)]$. Therefore,
    \begin{equation*}
      \mathbb{E}[\eta_{k}|\mathcal{H}_{k}] = 0
    \end{equation*}
    The condition $\mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}]<\infty$ can also be satisfied if
    $\|\nabla_{\omega}f(\omega,x_{k})\|<\infty$. Since the three conditions in theorem (\ref{rm:1}) are satisfied, the
    convergence of the SGD algorithm follows.
  \end{proof}

  \subsection{Convergence pattern of SGD}

    The convergence of SGD has been confirmed. Here we demonstrate the interesting convergence pattern of SGD. The relative
    error between the stochastic gradient and the true gradient can be defined as:
    \begin{equation*}
      \delta_{k} \triangleq \frac{\|\nabla_{\omega}f(\omega,X) -
        \mathbb{E}[\nabla_{\omega}f(\omega,X)]\|}{\|\mathbb{E}[\nabla_{\omega}f(\omega,X)]\|}
    \end{equation*}
    Since $\omega^{*}$ is the optimal solution, $\mathbb{E}[\nabla_{\omega}f(\omega^{*},X)]=0$, then the relative error can
    be rewritten as using mean value theorem:
    \begin{align*}
      \delta & = \frac{\|\nabla_{\omega}f(\omega,X) - \mathbb{E}[\nabla_{\omega}f(\omega,X)]\|}{\|\mathbb{E}[\nabla_{\omega}f(\omega,X)]-\mathbb{E}[\nabla_{\omega}f(\omega^{*},X)]\|}                                    \\
             & = \frac{\|\nabla_{\omega}f(\omega,X) - \mathbb{E}[\nabla_{\omega}f(\omega,X)]\|}{\|\mathbb{E}[\nabla_{\omega}^{2}f(\tilde{\omega},X)(\omega-\omega^{*})]\|} \quad (\tilde{\omega} \in [\omega,\omega^{*}]) \\
    \end{align*}
    If $f$ is strict convex such that $\nabla_{\omega}^{2}f(\omega,X) \ge c >0$,
    \begin{align*}
      \|\mathbb{E}[\nabla_{\omega}^{2}f(\tilde{\omega},X)(\omega-\omega^{*})]\| & =
      \|\mathbb{E}[\nabla_{\omega}^{2}f(\tilde{\omega},X)]\|\|(\omega-\omega^{*})\| \\ & \ge c\|(\omega-\omega^{*})\|
    \end{align*}
    So the relative error
    \begin{equation*}
      \delta \le \frac{\|\nabla_{\omega}f(\omega,X) - \mathbb{E}[\nabla_{\omega}f(\omega,X)]\|}{c\|(\omega-\omega^{*})\|}
    \end{equation*}
    From the inequality above, the relative error $\delta$ is inversely proportional to $\|\omega-\omega^{*}\|$. As a
    result, if $\omega_{k}$ is far from $\omega^{*}$, relative error is small and SGD behaves like gradient descent so that
    $\omega_{k}$ quickly converge to $\omega^{*}$. When $\omega_{k}$ is close to $\omega^{*}$, the relative error is large
    and the convergence may exhibit more randomness.

  \subsection{BGD, SGD and mini-batch GD}

    Those three concept are very simple. At one step, BGD uses the whole samples, SGD just uses a single sample, whereas
    mini-batch GD(MBGD) use a few more samples. In particular, suppose $\omega^{*}$ is solution for minimizing
    $J(\omega)=\mathbb{E}[f(\omega,X)]$. Given a set of random samples $\{x_{i}\}_{i=1}^{n}$, the BGD, SGD and MBGD for
    solving the problem are, respectively,
    \begin{align*}
      \omega_{k+1} = \omega_{k} - \alpha_{k}\frac{1}{n}\sum_{i=1}^{n}\nabla_{w}f(\omega,x_{i}) \quad (BGD) \\
      \omega_{k+1} = \omega_{k} - \alpha_{k}\nabla_{w}f(\omega,x_{i}) \quad (SGD)                          \\
      \omega_{k+1} = \omega_{k} - \alpha_{k}\frac{1}{m}\sum_{i=1}^{m}\nabla_{w}f(\omega,x_{i}) \quad (MBGD)
    \end{align*}
    When n is large, $1/n \sum_{i=1}^{n} \nabla_{\omega}f(\omega,x_{i})$ is close to $\mathbb{E}[\nabla_{\omega}f(\omega,
        X)]$ in BGD. In the MBGD, $m$ is the size of the subset $\{x_{i}\}_{i=1}^{n}$ that is collected at step $k$. MBGD is
    an intermediate version between SGD and BGD which exhibit less randomness than SGD and using smaller set than BGD. Of
    course, MBGD in general has a faster convergence speed than SGD since $1/m \sum_{i=1}^{m}
      \nabla_{\omega}f(\omega,x_{i})$ is closer to $\mathbb{E}[\nabla_{\omega}f(\omega, X)]$ than
    $\nabla_{\omega}f(\omega,x_{i})$.

  \subsection{A deterministic formulation of SGD}

    This is the last topic which I think is very interesting. "Stochastic" in SGD is because $\{x_{i}\}$ is a stochastic
    sequence. However, if the sequence is a deterministic sequence, can we use algorithm below to find the optimal solution
    for $\min_{\omega} J(\omega) = \frac{1}{n}\sum_{i=1}^{n}f(\omega,x_{i})$?
    \begin{equation}
      \label{eq:dSGD}
      \omega_{k+1} = \omega_{k} - \alpha_{k} \nabla_{\omega}f(\omega_{k},x_{k})
    \end{equation}
    The answer is yes! We just need a random variable $X$ of which sample space is $\{x_{i}\}_{i=1}^{n}$ and probability
    distribution is uniform such that $p(X=x_{i})=1/n$. Now the original problem is converted to
    \begin{equation*}
      \min_{\omega} J(\omega) = \mathbb{E}[f(\omega,X)] = \frac{1}{n}\sum_{i=1}^{n}f(\omega,x_{i})
    \end{equation*}
    This conversion is not approximate but  strict. Now the equation (\ref{eq:dSGD}) is an SGD algorithm, and it will
    converge if $x_{k}$ is independent and uniformly sampled from $\{x_{i}\}_{i=l}^{n}$. It should be noted that $x_{k}$ may
    have the same value in different step because it is randomly picked from set $\{x_{i}\}_{i=1}^{n}$.  
